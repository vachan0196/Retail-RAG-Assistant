ğŸ›ï¸ Retail RAG Assistant

An AI-powered Q&A assistant for retail knowledge bases.
Built with RAG (Retrieval-Augmented Generation), FAISS vector search, and modern LLMs (Cohere / OpenAI).

This project demonstrates real-world AI skills companies expect in 2025:

âœ… RAG pipelines (retriever + reranker + generator)

âœ… Vector databases (FAISS locally, Pinecone/Weaviate for production)

âœ… Semantic search with embeddings

âœ… Prompt/context engineering

âœ… LLMOps basics (evaluation, feedback logging)

âœ… Deployment via Streamlit + Docker

ğŸ“‚ Project Structure
retail-rag-assistant/
â”‚
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ rag.py                  # Retrieval + generation pipeline
â”œâ”€â”€ ingest.py               # Chunk & ingest documents
â”œâ”€â”€ index.py                # Embeddings + FAISS index
â”œâ”€â”€ eval.py                 # Simple evaluation scripts
â”œâ”€â”€ utils/                  # Helper functions
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ Dockerfile              # Docker setup
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â””â”€â”€ .env.example            # Environment template (no secrets)

âš¡ Setup
1) Clone the repo
git clone https://github.com/vachan0196/Retail-RAG-Assistant.git
cd Retail-RAG-Assistant

2) Create environment
conda create -n retail-rag python=3.11 -y
conda activate retail-rag
pip install -r requirements.txt

3) Configure environment variables

Copy the example env and fill in your keys:

cp .env.example .env

â–¶ï¸ Usage
Run ingestion (chunk documents)
python ingest.py

Build embeddings & FAISS index
python index.py embed
python index.py build

Test retrieval + RAG pipeline
python rag.py --cmd answer --q "What is the return window for returns?" --k 8 --rk 5

Launch Streamlit app
streamlit run app.py

ğŸ³ Run with Docker
docker build -t retail-rag .
docker run -p 8501:8501 retail-rag


App will be live at http://localhost:8501
.

ğŸ“Š Evaluation (LLMOps-lite)
python eval.py


Outputs:

Retrieval metrics (Hit@k, MRR)

Faithfulness check samples â†’ artifacts/eval/results.json

ğŸŒŸ Features

Semantic search with sentence-transformers/all-MiniLM-L6-v2

Cross-encoder reranking (ms-marco-MiniLM-L-6-v2)

Flexible generation (OpenAI GPT-4o-mini / Cohere command-r / offline fallback)

Streamlit chat UI with citations and sources

Clean modular design (easy to extend with Pinecone, Weaviate, or LangChain)

ğŸš€ Business Impact

This project shows how retail companies can:

Reduce support load by automating FAQs (returns, warranty, delivery)

Provide faster answers grounded in policies & product catalogues

Extend to sales insights, recommendations, and seasonal analysis

ğŸ“Œ Notes

Never commit .env with real keys. Only use .env.example.

Models are cached locally (HF_CACHE_DIR), not stored in GitHub.

ğŸ‘¤ Author

Vachan Sardar
Data Science, ML Engineering, AI Agents

LinkedIn www.linkedin.com/in/vachan-sardar
